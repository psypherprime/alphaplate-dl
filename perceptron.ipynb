{
 "cells": [
  {
   "cell_type": "code",
   "id": "264163b7256360f8",
   "metadata": {
    "collapsed": true
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_linear_regression(X_batch: np.ndarray, y_batch: np.ndarray, weights: dict[str, np.ndarray]) -> tuple[float, dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Implementation of forward pass of the perceptron.\n",
    "    Here, We are calculating the matrices to get the prediction label.\n",
    "    \"\"\"\n",
    "    # Checking if the number of labels is equal to number of inputs\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    # Checking the dimensions for matrix multiplication\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    # Checking that B is 1x1 ndarray\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1] == 1\n",
    "\n",
    "    N = np.dot(X_batch, weights['W'])\n",
    "    P = N + weights['B']\n",
    "\n",
    "    loss = np.mean(np.square(y_batch - P))\n",
    "    forward_info: dict[str, np.ndarray] = {'X': X_batch, 'N': N, 'P': P, 'y': y_batch}\n",
    "\n",
    "    return loss, forward_info"
   ],
   "id": "8b7561153d0eb908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def loss_gradients(forward_info: dict[str, np.ndarray], weights: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    batch_size  = forward_info['X'].shape[0]\n",
    "    \"\"\"\n",
    "    dLdW = dLdP * dPdN * dNdW\n",
    "    dLdP = dLdP * dPdB * dBdB\n",
    "    \"\"\"\n",
    "    dLdP = -2 * (forward_info['y'] - forward_info['P'])\n",
    "    dPdN = np.ones_like(forward_info['N'])\n",
    "    dPdB = np.ones_like(weights['B'])\n",
    "\n",
    "    dLdN = dLdP * dPdN\n",
    "    dNdW = np.transpose(forward_info['X'], (1, 0)) # Check this\n",
    "\n",
    "    dLdW = np.dot(dNdW, dLdN)\n",
    "    dLdB = (dLdP * dPdB).sum(axis=0)\n",
    "\n",
    "    loss_gradients: dict[str, np.ndarray] = {'W': dLdW, 'B': dLdB}\n",
    "\n",
    "    return loss_gradients"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Batch = tuple[np.ndarray, np.ndarray]\n",
    "\n",
    "def generate_batch(X: np.ndarray, y: np.ndarray, start: int = 0, batch_size: int = 32) -> Batch:\n",
    "    # Checking for perceptron requirements and same units\n",
    "    assert X.ndim == y.ndim == 2\n",
    "    \n",
    "    # Controlling overflow\n",
    "    if start + batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0] - start\n",
    "    \n",
    "    X_batch, y_batch = X[start:start + batch_size], y[start:start + batch_size]\n",
    "    \n",
    "    return X_batch, y_batch"
   ],
   "id": "186343355267cfb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_loss(X: np.ndarray, y:np.ndarray, weights: dict[str, np.ndarray]) -> tuple[dict[str, np.ndarray], float]:\n",
    "    N = np.dot(X, weights['W'])\n",
    "    P = N + weights['B']\n",
    "    loss = np.mean(np.square(y - P))\n",
    "\n",
    "    forward_info: dict[str, np.ndarray] = {'X': X, 'N': N, 'P': P, 'y': y}\n",
    "\n",
    "    return forward_info, loss"
   ],
   "id": "87fcbf2de82ebec8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def init_weights(n_in: int) -> dict[str, np.ndarray]:\n",
    "    W = np.random.randn(n_in, 1)\n",
    "    B = np.random.randn(1, 1)\n",
    "    \n",
    "    weights = {'W': W, 'B': B}\n",
    "    \n",
    "    return weights"
   ],
   "id": "f7fa137a5acce0e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(X: np.ndarray, y: np.ndarray, epochs: int = 100, batch_size: int = 32, learning_rate: float = 0.01,\n",
    "          return_losses:bool = False, return_weights:bool = False, seed: int = 0\n",
    "          ):\n",
    "\n",
    "    if seed != 0:\n",
    "        np.random.seed(seed)\n",
    "    # Shuffling Data\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X, y = X[perm], y[perm]\n",
    "\n",
    "    start = 0\n",
    "    weightst = init_weights(X.shape[1])\n",
    "\n",
    "    if return_losses:\n",
    "        losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        if start >= X.shape[0]:\n",
    "            perm = np.random.permutation(X.shape[0])\n",
    "            X, y = X[perm], y[perm]\n",
    "            start = 0\n",
    "\n",
    "        X_batch, y_batch = generate_batch(X, y, start, batch_size)\n",
    "        start += batch_size\n",
    "\n",
    "        forward_info, loss = forward_loss(X_batch, y_batch, weightst)\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_grads = loss_gradients(forward_info, weightst)\n",
    "        for key in weightst.keys():\n",
    "            weightst[key] -= (learning_rate * loss_grads[key])\n",
    "\n",
    "        # print(weightst['W'], (learning_rate * loss_grads['W']), weightst['W'] - (learning_rate * loss_grads['W']))\n",
    "\n",
    "    if return_weights:\n",
    "        return losses, weightst\n",
    "\n",
    "    return None"
   ],
   "id": "7f6c63e4b75d690b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target"
   ],
   "id": "2ccde64eae945243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "X = s.fit_transform(X)"
   ],
   "id": "626c7ec4e82be05f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "divd = int(0.7*len(X))\n",
    "X_train, X_test, y_train, y_test = X[:divd], X[divd:], y[:divd], y[divd:]\n",
    "\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)"
   ],
   "id": "cce288053900f0f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_info = train(X_train, y_train, epochs=100, batch_size=32, learning_rate=0.001, return_losses=True, return_weights=True)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]"
   ],
   "id": "a14136fd6dd5e8a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "losses",
   "id": "e7cf011978e437d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "weights",
   "id": "7b5b552288ec9d4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f204938f89789e22",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
